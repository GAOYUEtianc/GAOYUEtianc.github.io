Generally speaking, goal of reinforcement learning is to find an optimal behaviour strategy which maximizes rewards. Policy gradient methods are essential techniques in RL that directly optimize the parameterized policy by using an estimator of the gradient of the expected cost. In policy gradient methods, large step often leads to disasters, hence we should avoid  parameter updates that change the policy too much at one step. TRPO can improve training stability by constraining our step length to be within a “trust region”. However, since TRPO needs a KL divergence constraint on the size of update in each iteration, the implementation is complicated, hence another method, PPO, which simplifies TRPO a lot while emulating it, was proposed. Based on the fact that a certain surrogate objective forms a pessimistic bound on the performance of the policy, instead of using a KL divergence constraint, PPO uses a penalty.
