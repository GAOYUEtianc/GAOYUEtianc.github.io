<p>This is my very first blog, and from now on I’ll try my best to post a blog every two weeks, I think it’s a good way to force myself to have a thorough understanding of papers I read and also helps improving my writing skills. Topics may include RL, ML, bandit, DP and so on. Welcome to comment, criticize, and offer suggestions :)</p>
<p>Welcome to my blog :) I’ll share my reading summaries and maybe also some random research thoughts/feelings, hopefully, in every two weeks. The topics may include reinforcement learning, bandit, machine learning, differential privacy and so on. You’re more than welcome to comment on my posts, and I’ll be even happier if you could recommend papers or books to me. Besides ML &amp; RL, I also enjoy reading geography/literature/sci-fi.<br />
English is my second language, I apologize if the grammar isn’t up to scratch on some of my posts.</p>

<h1 id="introduction">Introduction</h1>
<p>Generally speaking, goal of reinforcement learning is to find an optimal behaviour strategy which maximizes rewards. Policy gradient methods are essential techniques in RL that directly optimize the parameterized policy by using an estimator of the gradient of the expected cost. In policy gradient methods, large step often leads to disasters, hence we should avoid parameter updates that change the policy too much at one step. TRPO can improve training stability by constraining our step length to be within a “trust region”. However, since TRPO needs a KL divergence constraint on the size of update in each iteration, the implementation is complicated, hence another method, PPO, which simplifies TRPO a lot while emulating it, was proposed. Based on the fact that a certain surrogate objective forms a pessimistic bound on the performance of the policy, instead of using a KL divergence constraint, PPO uses a penalty.</p>
<h1 id="trpo">TRPO</h1>
<p><strong>Keywords :</strong> MM algorithm, monotonic improvement, 2nd-order optimization, conjugate gradient method,</p>
<h2 id="introduction-to-trpo">Introduction to TRPO</h2>
<p>An essential idea of TRPO is monotinic improvement guarantee for policy, and to achieve monotonic improvement, an intuitive thought is to decompose the reward (or cost) of new policy into reward (or cost) of old policy plus an ”advantage term”.<br />
Let <span class="math inline">𝒮</span> be the finite set of states, <span class="math inline">𝒜</span> be the finite set of actions, <span class="math inline"><em>P</em> : 𝒮 × 𝒜 × 𝒮 → ℝ</span> be the transition probability distribution, <span class="math inline"><em>c</em> : 𝒮 → ℝ</span> be the cost function, <span class="math inline"><em>ρ</em><sub>0</sub> : 𝒮 → ℝ</span> be the distribution of the initial state <span class="math inline"><em>s</em><sub>0</sub></span>, <span class="math inline"><em>γ</em> ∈ (0, 1)</span> be the discount factor.<br />
Here are some standard definitions of the state-action value function <span class="math inline"><em>Q</em><sub><em>π</em></sub></span>, the value function <span class="math inline"><em>V</em><sub><em>π</em></sub></span>, and the advantage function <span class="math inline"><em>A</em><sub><em>π</em></sub></span> : <br /><span class="math display">$$\begin{aligned}
Q_{\pi} (s_{t}, a_{t}) &amp;=&amp; \mathbb{E}_{s_{t+1}, a_{t+1}, ...}\left[\sum_{l=0}^{\infty}\gamma^{l}c(s_{t+l})\right]\\
V_{\pi}(s_{t}) &amp;=&amp; \mathbb{E}_{a_{t},s_{t+1},...}\left[\sum_{l=0}^{\infty}\gamma^{l}c(s_{t+l})\right]\\
A_{\pi}(s,a) &amp;=&amp; Q_{\pi}(s,a) - V_{\pi}(s), \;where\; a_{t}\sim \pi(a_{t}, s_{t}), s_{t+1}\sim P[s_{t+1}|s_{t},a_{t}]\end{aligned}$$</span><br /><br />
Kakade &amp; Langford raised up an identity that express the expected cost of another policy <span class="math inline"><em>π̃</em></span> in terms of the accumulated advantage over <span class="math inline"><em>π</em></span> : <br /><span class="math display">$$\begin{aligned}
\eta(\tilde{\pi}) &amp;=&amp; \eta(\pi) + \mathbb{E}_{s_{0},a_{0},s_{1},a_{1},...}\left[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_{t},a_{t})\right]\end{aligned}$$</span><br /> where <span class="math inline"><em>s</em><sub>0</sub> ∼ <em>ρ</em><sub>0</sub>(<em>s</em><sub>0</sub>)</span>, <span class="math inline"><em>a</em><sub><em>t</em></sub> ∼ <em>π̃</em>(<em>a</em><sub><em>t</em></sub>|<em>s</em><sub><em>t</em></sub>)</span>, <span class="math inline"><em>s</em><sub><em>t</em> + 1</sub> ∼ <em>P</em>[<em>s</em><sub><em>t</em> + 1</sub>|<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>]</span>.<br />
The proof is as follows (let <span class="math inline"><em>τ</em>|<em>π̃</em></span> denote the trajectory <span class="math inline"><em>τ</em> = (<em>s</em><sub>0</sub>, <em>a</em><sub>0</sub>, <em>s</em><sub>1</sub>, <em>a</em><sub>1</sub>)</span> generated from <span class="math inline"><em>π̃</em></span>):<br /><span class="math display">$$\begin{aligned}
&amp;&amp;\mathbb{E}_{\tau|\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_{t},a_{t})\right] \\
&amp;=&amp; \mathbb{E}_{\tau|\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^{t} (r(s_{t}) +\gamma V_{\pi}(s_{t+1})-V_{\pi}(s_{t}))\right]\\
&amp;=&amp; \mathbb{E}_{\tau|\tilde{\pi}}\left[-V_{\pi}(s_{0})+\sum_{t=0}^{\infty}\gamma^{t}\cdot r(s_{t})\right]\;\;\;By\;expanding\;terms\\
&amp;=&amp; -\mathbb{E}_{s_{0}}\left[V_{\pi}(s_{0})\right] 
 + \mathbb{E}_{\tau|\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^{t}\cdot r(s_{t})\right]\\
&amp;=&amp; -\eta(\pi) + \eta(\tilde{\pi})\end{aligned}$$</span><br /> Let <span class="math inline"><em>ρ</em><sub><em>π</em></sub>(<em>s</em>) = <em>P</em>[<em>s</em><sub>0</sub> = <em>s</em>] + <em>γ</em><em>P</em>[<em>s</em><sub>1</sub> = <em>s</em>] + <em>γ</em><sup>2</sup><em>P</em>[<em>s</em><sub>2</sub> = <em>s</em>] + ...</span> be the (unnormalized) discounted visitaion frequencies. Then<br /><span class="math display">$$\begin{aligned}
\eta(\tilde{\pi}) &amp;=&amp; \eta(\pi) +\mathbb{E}_{\tau|\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_{t},a_{t})\right]\\
&amp;=&amp; \eta(\pi) + \sum_{t=0}^{\infty}\sum_{s}P[s_{t}=s|\tilde{\pi}]\sum_{a}\tilde{\pi}(a|s)\gamma^{t}A_{\pi}(s,a)\\
&amp;=&amp; \eta(\pi) + \sum_{s}\sum_{t=0}^{\infty}\gamma^{t}P[s_{t}=s|\tilde{\pi}]\sum_{a}\tilde{\pi}(a|s)A_{\pi}(s,a)\\
&amp;=&amp; \eta(\pi) + \sum_{s}\rho_{\tilde{\pi}}(s)\sum_{a}\tilde{\pi}(a|s)A_{\pi}(s,a)\end{aligned}$$</span><br /><br />
However, since we don’t have <span class="math inline"><em>ρ</em><sub><em>π̃</em></sub></span>, eqn (13) is difficult to optimize, hence, we introduce a local approximation to <span class="math inline"><em>η</em></span>, and note that this is the first ’approximation trick’ in TRPO :<br /><span class="math display">$$\begin{aligned}
L_{\pi}(\tilde{\pi}) = \eta(\pi) + \sum_{s}\rho_{\pi}(s)\sum_{a}\tilde{\pi}(a|s)A_{\pi}(s,a)\end{aligned}$$</span><br /> Compared to eqn (13), <span class="math inline"><em>L</em><sub><em>π</em></sub></span> uses <span class="math inline"><em>ρ</em><sub><em>π</em></sub></span> as a local approximation of <span class="math inline"><em>ρ</em><sub><em>π̃</em></sub></span>, intuitively this is reasonable since within 1 iteration, the new policy won’t be that different from the old one, so the visitation frequencies under those two policies would be similar. More precisely, if the parameterized policy <span class="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>)</span> is a differentiable function of the parameter vector <span class="math inline"><em>θ</em></span>, then <span class="math inline"><em>L</em><sub><em>π</em></sub></span> matches <span class="math inline"><em>η</em></span> to the first order, i.e., for any <span class="math inline"><em>θ</em><sub>0</sub></span>, <br /><span class="math display">$$\begin{aligned}
L_{\pi_{\theta_{0}}}(\pi_{\theta_{0}}) &amp;=&amp; \eta(\pi_{\theta_{0}})\\
\nabla_{\theta}L_{\pi_{\theta_{0}}}(\pi_{\theta})|_{\theta=\theta_{0}} &amp;=&amp; \nabla_{\theta}\eta(\pi_{\theta})|_{\theta=\theta_{0}}\end{aligned}$$</span><br /> According to the thoughts behind Minorize-Maximization algorithms, by iteratively maximizing lower bound function of the expected reward, an algorithm can guarantee that any policy update always improve the expected reward. Let <br /><span class="math display">$$\begin{aligned}
\pi^{'}&amp;=&amp;\underset{\pi^{'}}{\argmax}\;L_{\pi_{old}}(\pi^{'})\\
\pi_{new}(a|s) &amp;=&amp; (1-\alpha)\pi_{old}(a|s)+\alpha\pi^{'}(a|s)\end{aligned}$$</span><br /> Kakade &amp; Langford derived the following lower bound :<br /><span class="math display">$$\begin{aligned}
\eta(\pi_{new}) &amp;\geq &amp; L_{\pi_{old}}(\pi_{new}) - \frac{2\epsilon\gamma}{(1-\gamma)^{2}}\alpha^{2}\\
where\;\epsilon &amp;=&amp; \underset{s}{\max}\;|\mathbb{E}_{a\sim\pi^{'}(a|s)}[A_{\pi}(s,a)]|\end{aligned}$$</span><br /> However, mixer policies are rarely used in practice, so based on this bound, Schulman et al. extended this to general stochastic policies by replacing <span class="math inline"><em>α</em></span> in eqn (19) by <span class="math inline"><em>D</em><sub><em>T</em><em>V</em></sub><sup><em>m</em><em>a</em><em>x</em></sup>(<em>π</em><sub><em>o</em><em>l</em><em>d</em></sub>, <em>π</em><sub><em>n</em><em>e</em><em>w</em></sub>)</span>, where <span class="math inline">$D_{TV}^{max}(\pi,\tilde{\pi})=\underset{s}{\max}\;D_{TV}(\pi(\cdot|s)||\tilde{\pi}(\cdot|s))$</span> and <span class="math inline">$D_{TV}(p||q)=\frac{1}{2}\sum_{i}|p_{i}-q_{i}|$</span> be the total variation divergence. Pollard has proved that <span class="math inline">$D_{KL}^{max}(\pi,\tilde{\pi})=\underset{s}{\max}\;D_{KL}(\pi(\cdot|s)||\tilde{\pi}(\cdot|s))$</span>. Hence in the lower bound we can replace <span class="math inline"><em>D</em><sub><em>T</em><em>V</em></sub><sup><em>m</em><em>a</em><em>x</em></sup>(<em>π</em><sub><em>o</em><em>l</em><em>d</em></sub>, <em>π</em><sub><em>n</em><em>e</em><em>w</em></sub>)</span> by <span class="math inline"><em>D</em><sub><em>K</em><em>L</em></sub><sup><em>m</em><em>a</em><em>x</em></sup>(<em>π</em><sub><em>o</em><em>l</em><em>d</em></sub>, <em>π</em><sub><em>n</em><em>e</em><em>w</em></sub>)</span>.<br />
An intuitive thought is that by iteratively returning the policies maximizing <span class="math inline">$L_{\pi_{old}}(\pi)-\frac{2\epsilon\gamma}{(1-\gamma)^{2}}\;D_{KL}^{max}(\pi_{old}, \pi)$</span>, we can guarantee monotonic improvement. However, computing the maximum KL divergence needs to iterate over all states, which is super intractable, fortunately, Schulman et al. showed that here mean KL divergence over state space is a valid approximation to maximum KL divergence <span style="color: red">empirically. (I have a doubt here, I didn’t find any theoretical evidence that replacing max KL divergence by average KL divergence is ’theoretically valid’. The paper only mentioned that under those two types of divergence, the algorithms have similar performance empirically.)</span><br />
Finally, we arrive at the optimization problem :<br /><span class="math display">$$\begin{aligned}
\underset{\theta}{maximize}\left[L_{\theta_{old}}(\theta) - \frac{2\epsilon\gamma}{(1-\gamma)^{2}}\cdot\bar{D}_{KL}^{\rho_{old}}(\theta_{old}, \theta)\right]\end{aligned}$$</span><br /> where <span class="math inline"><em>D̄</em><sub><em>K</em><em>L</em></sub><sup><em>ρ</em></sup>(<em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>) = 𝔼<sub><em>s</em> ∼ <em>ρ</em></sub>[<em>D</em><sub><em>K</em><em>L</em></sub>(<em>π</em><sub><em>θ</em><sub>1</sub></sub>(⋅|<em>s</em>)||<em>π</em><sub><em>θ</em><sub>2</sub></sub>(⋅|<em>s</em>))]</span>.<br />
There are two basic iterative approaches to find a local minimum/maximum of an objective function, line search and trust region. The method adopted in this paper is trust region, i.e.,determine the maximum step size we want to explore then locate the optimal point within this trust region :<br /><span class="math display">$$\begin{aligned}
 \underset{\theta}{maximize}\; \sum_{s}\rho_{\theta_{old}}(s)\sum_{a}\pi_{\theta}(a|s)A_{\theta_{old}}(s,a)\\
 subject\;to\; \bar{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \leq \delta\end{aligned}$$</span><br /> Consider the case when we are doing off-policy RL, the policy <span class="math inline"><em>q</em></span> used for collecting trajectories is different from the policy to optimize. The mismatch between the training data distribution and the true policy state distribution is compensated by importance sampling estimator. Note that since true rewards are usually unknown, we use an estimated advantage <span class="math inline"><em>Â</em>( ⋅ )</span> (by performing a rollout) instead of <span class="math inline"><em>A</em>( ⋅ )</span> :<br /><span class="math display">$$\begin{aligned}
 &amp; \sum_{s}\rho_{\theta_{old}}(s)\sum_{a}\pi_{\theta}(a|s)\hat{A}_{\theta_{old}}(s,a) \\
 &amp;= \sum_{s}\rho_{\theta_{old}}(s)\sum_{a}q(a|s)\frac{\pi_{\theta}(a|s)}{q(a|s)}\hat{A}_{\theta_{old}}(s,a)\\
 &amp;= \mathbb{E}_{s\sim \rho_{old},a\sim q}\left[\frac{\pi_{\theta}(a|s)}{q(a|s)}\hat{A}_{\theta_{old}}(s,a)\right]\end{aligned}$$</span><br /></p>
<p>In continuous control problems, it’s better to use <span class="math inline"><em>π</em><sub><em>θ</em><sub><em>o</em><em>l</em><em>d</em></sub></sub></span> as behaviour policy.<br />
After getting the objective function, we can solve this optimization problem with a 2nd order approximation of the KL divergence, 1st order approximation of loss and natural gradient descent. Note that the natural policy gradient descent requires computing of Fisher Information Matrix and its inverse, which is expensive.</p>
<h1 id="ppo">PPO</h1>
<p>A major disadvantage of TRPO is that it’s computationally expensive, Schulman et al. proposed proximal policy optimization (PPO) to simplify TRPO by using a clipped surrogate objective while retaining similar performance. Compared to TRPO, PPO is simpler, faster, and more sample efficient.<br />
Let <span class="math inline">$r_{t}(\theta) = \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$</span>, then the surrogate objective of TRPO is <br /><span class="math display">$$\begin{aligned}
L^{CPI}(\theta) = \hat{\mathbb{E}}_{t}\left[r_{t}(\theta)\hat{A}_{t}\right]\end{aligned}$$</span><br /> Note that without a limitation on the distance between <span class="math inline"><em>θ</em></span> and <span class="math inline"><em>θ</em><sub><em>o</em><em>l</em><em>d</em></sub></span>, maximizing <span class="math inline"><em>L</em><sup><em>C</em><em>P</em><em>I</em></sup></span> will lead to excessively large parameter updates and big policy ratios. PPO penalize changes to policy by forcing <span class="math inline"><em>r</em><sub><em>t</em></sub>(<em>θ</em>)</span> to stay within interval <span class="math inline">[1 − <em>ϵ</em>, 1 + <em>ϵ</em>]</span> (where <span class="math inline"><em>ϵ</em></span> is a hyperparameter). The objective function is : <br /><span class="math display">$$\begin{aligned}
L^{CLIP}(\theta) = \hat{\mathbb{E}}_{t}\left[\min\; (r_{t}(\theta)\hat{A}_{t},\; clip(r_{t}(\theta),1-\epsilon, 1+\epsilon)\hat{A}_{t})\right]\end{aligned}$$</span><br /> where the function <span class="math inline"><em>c</em><em>l</em><em>i</em><em>p</em>(<em>r</em><sub><em>t</em></sub>(<em>θ</em>), 1 − <em>ϵ</em>, 1 + <em>ϵ</em>)</span> clips the ratio <span class="math inline"><em>r</em><sub><em>t</em></sub>(<em>θ</em>)</span> within <span class="math inline">[1 − <em>ϵ</em>, 1 + <em>ϵ</em>]</span>. The objective function takes the minimum between the clipped and unclipped objective, so the final objective is a pessimistic bound on the unclipped one. Gradient Descent like Adam can be used to optimize it.<br />
When applying PPO on the network architecture with shared parameters for both policy and value function, the objective function can be further augmented with an error term on the value estimation and an entropy bonus to ensure sufficient exploration : <br /><span class="math display">$$\begin{aligned}
L_{t}^{LIP+VF+S}(\theta) = \hat{\mathbb{E}}_{t}\left[L_{t}^{CLIP}(\theta) - c_{1}(V_{\theta}(s_{t}) - V_{t}^{target})^{2} + c_{2}S[\pi_{\theta}](s_{t})\right]\end{aligned}$$</span><br /> where <span class="math inline"><em>c</em><sub>1</sub>, <em>c</em><sub>2</sub></span> are coefficients, and <span class="math inline"><em>S</em></span> is the entropy bonus.</p>
<h2 id="controversies-in-ppo">Controversies in PPO</h2>
<p>PPO is controversial though, it has shown great practical promise, however, there are works raising doubts about PPO. A work called ’Implementation Matters In Deep Policy Gradients : A Case Study On PPO And TRPO’ investigated the consequences of ’code-level optimizations’ in detail. Concretely, PPO’s code-optimizations are significantly more important in terms of final reward, instead of the choice of general training algorithm (TRPO vs. PPO), contradicting the belief that ’clipping tech’ is the key innovation of PPO. Also, PPO enforces trust region by code-level optimizations instead of the clipping technique. Moreover, the clipping technique may not be necessary, PPO-NoCLIP algorithm, which uses code-level optimizations but no clipping mechanism, achieves similar results to PPO in terms of benchmark performance.</p>
<h1 id="reflections">Reflections</h1>
<p>This issue in PPO points to a broader problem: we don’t really understand how the parts comprising deep RL algorithms impact agent training, either as individuals or as a whole. When designing deep RL algorithms, it’s necessary for us to understand precisely how each component impacts agent training and designing algorithms in a modular manner.<br />
We’ve also noticed that although in PPO the clipping technique is designed to limit the distance between <span class="math inline"><em>θ</em></span> and <span class="math inline"><em>θ</em><sub><em>o</em><em>l</em><em>d</em></sub></span>, actually clipping technique does not enforce the KL region as it’s supposed to. Then an overarching question is : To what degree does current practice in deep RL reflect the principles informing its development? Motivated by this question, the work ’A closer look at deep policy gradients’ proposed a fine-grained analysis of state-of-the-art methods, and (sadly), the results show that the behaviour of deep policy gradient algorithms often deviates from the prediction of their motivating framework :</p>
<ol>
<li><p>Deep policy gradient methods operate with relatively poor estimates of the gradient, especially as task complexity increases and as training progresses. Better gradient estimates can require lower learning rates and can induce degenerate agent behaviour.</p></li>
<li><p>As training progresses, the surrogate objective becomes much less predictive of the true reward in the relevant sample regime, the underlying optimization landscape can be misleading.</p></li>
<li><p>Learned value estimators does not accurately model the true value function, and the value networds reduce gradient estimation variance to a significantly smaller extent than the true value.</p></li>
</ol>
<p>In conclusion, we need a more fine-grained understanding of deep RL algorithms, and to close the gap between the theory inspiring algorithms and the actual mechanisms, we need to either develop methods intimately bound up with theory, or build theory that can capture what makes existing policy gradient methods successful.<br />
<span style="color: blue">Thanks Alex Lewandowski for recommending those brilliant papers!</span></p>
<h1 id="references">References</h1>
<p><span>99</span> John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning (ICML), pages 1889–1897, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry. Implementation Matters in Deep RL: A Case Study on PPO and TRPO. https://openreview.net/forum?id=r1etN1rtPB. Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry. A Closer Look at Deep Policy Gradients. https://openreview.net/forum?id=ryxdEkHtPS</p>
