<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.65.0" />

  <title>2020 Resolutions &middot; Anna&#39;s Web</title>

  <meta name="description" content="" />

  
  <meta property="og:locale" content="en"/>

  
  <meta property="og:image" content="https://www.gravatar.com/avatar/c3c54f26563752e0f84f5cf27c7d72ea?s=400&d=mp">

  
  <meta property="og:site_name" content="Anna&#39;s Web"/>
  <meta property="og:title" content="2020 Resolutions"/>
  <meta property="og:description" content="TRPO and PPO &ndash; A Reading Summary Introduction Generally speaking, goal of reinforcement learning is to find an optimal behaviour strategy which maximizes rewards."/>
  <meta property="og:url" content="https://GAOYUEtianc.github.io/blogs/posts/2020-resolutions/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2020-02-21T00:00:00Z"/>
  <meta property="article:modified_time" content="2020-02-21T00:00:00Z"/>
  <meta property="article:author" content="Gao Yue (Anna)">
  
  
  

  <script type="application/ld+json">
  {
    "@context" : "http://schema.org",
    "@type" : "Blog",
    "name": "Anna's Web",
    "url" : "https://GAOYUEtianc.github.io/blogs/",
    "image": "https://www.gravatar.com/avatar/c3c54f26563752e0f84f5cf27c7d72ea?s=400&d=mp",
    "description": ""
  }
  </script>

  
  <script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "name": "2020 Resolutions",
    "headline": "2020 Resolutions",
    "datePublished": "2020-02-21T00:00:00Z",
    "dateModified": "2020-02-21T00:00:00Z",
    "author": {
      "@type": "Person",
      "name": "Gao Yue (Anna)",
      "url": "https://GAOYUEtianc.github.io/blogs/"
    },
    "image": "https://www.gravatar.com/avatar/c3c54f26563752e0f84f5cf27c7d72ea?s=400&d=mp",
    "url": "https://GAOYUEtianc.github.io/blogs/posts/2020-resolutions/",
    "description": "TRPO and PPO \x26ndash; A Reading Summary Introduction Generally speaking, goal of reinforcement learning is to find an optimal behaviour strategy which maximizes rewards."
  }
  </script>
  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });

  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
  


  <link type="text/css"
        rel="stylesheet"
        href="https://GAOYUEtianc.github.io/blogs/css/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="https://GAOYUEtianc.github.io/blogs/css/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="https://GAOYUEtianc.github.io/blogs/css/hyde.css">

  


  <link type="text/css" rel="stylesheet" href="https://GAOYUEtianc.github.io/blogs/css/blog.css">

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
      <div class="author-image">
        <img src="https://www.gravatar.com/avatar/c3c54f26563752e0f84f5cf27c7d72ea?s=200&d=mp" class="img-circle img-headshot center" alt="Gravatar">
      </div>
      

      <h1>Anna&#39;s Web</h1>

      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="https://GAOYUEtianc.github.io/blogs/">Home</a>
        </li>
        <li>
          <a href="https://GAOYUEtianc.github.io/blogs/public/blogs/posts/"> Posts </a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://www.linkedin.com/in/yue-anna-gao-49a834107/" rel="me" title="Linkedin">
        <i class="fab fa-linkedin" aria-hidden="true"></i>
      </a>
      
      <a href="https://github.com/GAOYUEtianc" rel="me" title="GitHub">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://www.facebook.com/yue.gao.925" rel="me" title="Facebook">
        <i class="fab fa-facebook" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>


  <main class="content container">
  <div class="post">
  <h1>2020 Resolutions</h1>

  <div class="post-date">
    <time datetime="2020-02-21T00:00:00Z">Feb 21, 2020</time> · 2 min read
  </div>

  <h1 id="trpo-and-ppo----a-reading-summary">TRPO and PPO &ndash; A Reading Summary</h1>
<h2 id="introduction">Introduction</h2>
<p>Generally speaking, goal of reinforcement learning is to find an optimal behaviour strategy which maximizes rewards. Policy gradient methods are essential techniques in RL that directly optimize the parameterized policy by using an estimator of the gradient of the expected cost. In policy gradient methods, large step often leads to disasters, hence we should avoid  parameter updates that change the policy too much at one step. TRPO can improve training stability by constraining our step length to be within a “trust region”. However, since TRPO needs a KL divergence constraint on the size of update in each iteration, the implementation is complicated, hence another method, PPO, which simplifies TRPO a lot while emulating it, was proposed. Based on the fact that a certain surrogate objective forms a pessimistic bound on the performance of the policy, instead of using a KL divergence constraint, PPO uses a penalty.</p>
<h2 id="trpo">TRPO</h2>
<p><strong>Keywords :</strong> MM algorithm, monotonic improvement, 2nd-order optimization, conjugate gradient method, Fisher Informational Matrix.</p>
<h3 id="introduction-to-trpo">Introduction to TRPO</h3>
<p>An essential idea of TRPO is monotinic improvement guarantee for policy, and to achieve monotonic improvement, an intuitive thought is to decompose the reward (or cost) of new policy into reward (or cost) of old policy plus an &lsquo;&lsquo;advantage term&rsquo;'.</p>
<p>Let $\mathcal{S}$ be the finite set of states, $\mathcal{A}$ be the finite set of actions, $P:\mathcal{S}\times \mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}$ be the transition probability distribution, $c:\mathcal{S}\rightarrow \mathbb{R}$ be the cost function, $\rho_{0}:\mathcal{S}\rightarrow\mathbb{R}$ be the distribution of the initial state $s_{0}$, $\gamma\in (0,1)$ be the discount factor.</p>

</div>



  </main>

  <head>{0xc000733680 0xc0004431e0}</head>
<footer>
  <div class="copyright">
    &copy; Gao Yue 2020 · <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  </div>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'],['\\(','\\)']],
displayMath: [['$$','$$'],],
processEscapes: true,
processEnvironments: true,
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
TeX: { equationNumbers: { autoNumber: "AMS" },
   extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});

MathJax.Hub.Queue(function() {



var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i < all.length; i += 1) {
  all[i].SourceElement().parentNode.className += ' has-jax';
}
});
</script>
</footer>

  




  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>        

  

  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-55543164-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</body>
</html>
